{
"date": "2022-01-20",
"title": "Dockerizing a Node+Express+Postgress App (with NGINX)",
"author": "Alex",
"description": "An exploration of using Docker to containerize a Node API",
"url": "docker-node-postgres"
}

# Dockerizing a Node+Express+Postgress App (with NGINX)

The following is a step by step guide to dockerizing a basic Node.js application. Before starting you should have a basic understanding of Containers and Images. Here is the basic layout of the app we will be moving into containers.

```bash
├── database
│   ├── DATA.sql
├── server
│   ├── index.js
├── package.json
```

`DATA.sql` is simply a dump of a POSTGRES database that is queried by a vanilla express app (`index.js`) that serves a handful of basic GET routes. A quick note about getting to this point, the DATA.sql file was created using the following command. The `no-privileges` and `no-owner` commands are important as we will be specifying a new super user when the POSTGRES database runs inside the docker container.

```bash
pg_dump -h localhost -U sieke --no-privileges --no-owner xxDBNAMExx >> DATA.sql
```

## What is the Goal?

We are going to take the following high level steps to turn our basic API into one that has 5 containers (below).

1. Set up a dockerfile to create a postgres docker image for running our database.
2. Set up a dockerfile to create a node docker image for running our API(s).
3. Create a docker-compose.yml configuration to "link" our 5 containers together.
4. Set up a dockerfile to create an nginx docker image for load balancer / reverse proxy.
5. Make some final modifications to our server to make sure it can connect to our new containerized database.
6. Use docker-compose to launch our images + containers.

<img src="../images/entries/docker-schema.png" width="400">

The first container will run NGINX and will be used as a load-balancing reverse-proxy that distributes 1/3 of requests to each of our servers. Our express API server will be deployed in 3 separate containers. Each of the 3 containers will point to a single Postgres database.

An added benefit of having incoming requests hit the NGINX server is the ease of adding a caching layer. A few lines added to the config yields incredibly flexible caching options.

## 1. Postgres Container

We're going to create filed called `Dockerfile` in the /database/ directory. This file will contain the instructions for creating our Postgres Image.

```bash
├── database
│   ├── DATA.sql
│   ├── Dockerfile
```

The Dockerfile should contain the following instructions.

```dockerfile
#database/Dockerfile
FROM postgres:latest

ENV POSTGRES_USER my-username
ENV POSTGRES_PASSWORD my-password
ENV POSTGRES_DB my-database

COPY DATA.sql /docker-entrypoint-initdb.d/
```

The Dockerfile will pull the latest postgres image. Setting the 3 environment variables will initialize the container with those a SQL user/pass and create a db. The my-database name should match the name of the local db that you used in your local application.

Finally, the Dockerfile copies the DATA.sql dump file into a special folder inside the instance. The script will automatically run when the instance starts.

## 2. API (Node + Express) Containers

We're going to create a second Dockerfile in the root directory of our project (same as package.json).

```dockerfile
#database/Dockerfile
FROM node:12
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
COPY .env_docker .env
ENV PORT=8080
EXPOSE 8080
CMD [ "npm", "start" ]
```

The above dockerfile contains the instructions for building our Node.js API. It creates the image from the base node image, sets the working directory, copies package.json from our local machine to the image, then runs npm install.

The `Copy . .` command will copy everything from our local project directory. Because we do not want to include things like node_modules and most importantly our database folder, we can add a `.dockerignore` file which functions the same as a .gitignore file.

`ENV PORT=8080 \n EXPOSE 8080` specifies that our webserver will run on port 8080 from inside the container. We will still need to map this internal port to an external port (which will happen later when we use docker-compose to create a multi-container app)

```dockerfile
#.dockerignore
node_modules/
database/
..
```

## 3. docker-compose, defining our services

Docker-compose is a tool for easily defining and running multi-container Docker applications, like this! All we need to do is create a `docker-compose.yml` file in our root directory that will define the names of our services.

```yml
version: '3.9'

services:
  db:
    build: database/
    ports:
      - '5432:5432'
  api1:
    environment:
      - MESSAGE=API1
    build: .
    ports:
      - '3001:8080'
  api2:
    environment:
      - MESSAGE=API2
    build: .
    ports:
      - '3002:8080'
  api3:
    environment:
      - MESSAGE=API3
    build: .
    ports:
      - '3003:8080'
  nginx:
    build: nginx/
    ports:
      - '8080:80'
    depends_on:
      - api1
      - api2
      - api3
```

Our 5 services will be called `db` `api1` `api2` `api3` and `nginx`. Each of our servers has port specifications that map the internal ports (8080 -> 3001/3002/3003) for our three APIs, and (80 -> 8080) for our nginx server.

Eventually when the containers are running on our local machine we will be able to access the application @ localhost:8080.

The build command here specifies where to look for the 3 dockerfiles we've created (we'll create the nginx one in a second).

The final property is `depends_on` which basically tells docker to wait to start nginx until after the three api services have started.

## 4. NGINX Container

We now need to create our final Dockerfile to define our NGINX container.

NGINX is super fast web server that we are going to use to serve 2 functions:

1. Load balancing, since we will eventually run our Node.js API x3, the NGINX server will load balance all requests, spreading them evenly across the 3 servers.
2. Caching - we can specify options for caching which will significantly speed up the rate at which our service can respond to requests.

We need to create a new directory at the root (same level as `server/` and `database/`) called `nginx`.

```bash
├── nginx
│   ├── nginx.conf
│   ├── Dockerfile
```

```bash
#nginx.conf
upstream loadbalancer {
  server 172.17.0.1:3001 weight=1;
  server 172.17.0.1:3002 weight=1;
  server 172.17.0.1:3003 weight=1;
}

proxy_cache_path /etc/nginx/cache keys_zone=my_cache:10m max_size=500m inactive=60m use_temp_path=off;

server {
    location / {
        proxy_cache my_cache;
        proxy_cache_valid 200 30s;
        proxy_cache_methods GET;
        proxy_cache_min_uses 1;
        proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504;
        proxy_pass http://loadbalancer;
    }
}
```

The `upstream` command defines the URLs where we want our load balancer to pass requests. `172.17.0.1` is the hard coded default IP inside our API containers, but you may need to change this. 3001-3003 are the ports we defined for our API services in the `docker-compose.yml`.

The proxy_cache\* instructions here define the options for our caching layer. You can read more about these options @ [Proxy Cache docs](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache).

The final required field is our reverse proxy definition. `proxy_pass http://{loadbalancer}` tells the server where to proxy requests defined in the `upstream` command above.

```dockerfile
#nginx/Dockerfile
FROM nginx
RUN rm /etc/nginx/conf.d/default.conf
COPY nginx.conf /etc/nginx/conf.d/default.conf
```

The NGINX Dockerfile creates an image from the nginx base. Then it removes and copies our conf file into the proper directory within the container. Thats it!

## Final Tweaks to our Node.js Server

At this point our directory structure should be the following:

```bash
├── database
│   ├── DATA.sql
│   ├── Dockerfile
├── nginx
│   ├── nginx.conf
│   ├── Dockerfile
├── server
│   ├── index.js
├── package.json
├── Dockerfile
├── .dockerignore
├── docker-compose.yml
├── .env_docker #not created yet, but we will now!
```

The two changes we need to make are:

1. Our `server/index.js` is currently pointing to our local instance of Postgres, we need to make sure when it starts running inside our container it will properly point to the containerized postgres.
2. When all the services start there will be a race condition between the APIs and the database. We need to add some retry logic to the servers to make sure they re-attempt to connect.

### Pointing to Containerized Postgres

In our main Dockerfile, the line `COPY .env_docker .env` copies the file .env_docker into our Nodejs image.

```bash
##/.env_docker
PGUSER=my-user
PGPASSWORD=my-password
PGHOST=db
PGPORT=5432
PGDATABASE=my-database
```

`my-user` `my-password` and `my-database` should match the properties you specified in the `database/Dockerfile`.

The big GOTHCA here is that PGHOST=db. Typically this would point to `localhost`. But since the db will be running inside a separate container, localhost will no longer work. Assigning `PGHOSE=db` is some docker-compose magic that will dynamically route requests to the host in our database container. _Make Sure_ that this value matches the name of the service defined in your docker-compose.yml.

### Retry logic in our server

I added the following logic to the API that will try to connect for 10:00 to the database. Note that `new Pool()` does not need parameters if they are defined as environment variables as they are in the .env_docker file above.

```javascript
const connect = async () => {
  let retries = 20;
  while (retries) {
    try {
      pool = new Pool();
      console.log('>>>CONNECTED TO DB');
      return true;
    } catch (err) {
      console.log('>>>>>ERROR CONNECTING TO DB, retrying:', retries);
      await sleep(30000);
    }
    retries--;
  }
  return false;
};
```

## MAGIC - time to LAUNCH our Containers.

Now that we've

1. Created our image definitions in the Dockerfiles
2. Created a docker-compose file
3. Tweaked our service to run in a containerized environment

Its time to run docker-compose

```bash
docker-compose up -d
```

`(-d)` option simply runs this in detached mode (as a background process).

Docker-compose will automatically name your containers after the root project directory. Running `docker ps` should show you your containers.

```bash
$ docker ps

CONTAINER ID   IMAGE                 COMMAND                  CREATED       STATUS       PORTS                    NAMES
0d1a8387d655   api2_nginx   "/docker-entrypoint.…"   5 hours ago   Up 5 hours   0.0.0.0:8080->80/tcp     api2-nginx-1
ad7726d65cef   api2_api1    "docker-entrypoint.s…"   5 hours ago   Up 5 hours   0.0.0.0:3001->8080/tcp   api2-api1-1
664ae6aae037   api2_api3    "docker-entrypoint.s…"   5 hours ago   Up 5 hours   0.0.0.0:3003->8080/tcp   api2-api3-1
d7b3d2d55dea   api2_db      "docker-entrypoint.s…"   5 hours ago   Up 5 hours   0.0.0.0:5432->5432/tcp   api2-db-1
5f67fd9c1388   api2_api2    "docker-entrypoint.s…"   5 hours ago   Up 5 hours   0.0.0.0:3002->8080/tcp   api2-api2-1
```

### Final Check

You should see your completed API at `localhost:8080`. You're getting a request back from the nginx server!
